{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863ce5dc",
   "metadata": {},
   "source": [
    " # using pre-trained word embeddings, IMDB sentiment detection data base\n",
    " \n",
    " \n",
    " \n",
    " word emmbeding: Word2Ves,\n",
    " \n",
    " dl link IMDB raw_database: https://ai.stanford.edu/~amaas/data/sentiment/ \n",
    " \n",
    " dl link e GloVe word embeding: nlp.stanford.edu/projects/glove/ \n",
    " \n",
    " 2 class (neg and pos),\n",
    " 25000 train data (12500 each class),\n",
    " 25000 test data (12500 each class),\n",
    " common words pas feature size =10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ca55184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "imdb_dir='./datasets/Imdb'\n",
    "train_dir=os.path.join(imdb_dir, 'train')\n",
    "\n",
    "texts = [] #liste text ha\n",
    "labels = [] # liste label ha\n",
    "\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name=os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name): \n",
    "        # for fname in directory -->  read each file \n",
    "        if fname[-4:] == '.txt': # if  file format is .txt \n",
    "            f = open (os.path.join(dir_name , fname), errors=\"ignore\") \n",
    "            texts.append(f.read()) \n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c46a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88584 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# tokenize and vectorize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "training_samples=12500 # \n",
    "validation_samples=10000 # 5000 each class\n",
    "\n",
    "maxlen=100 \n",
    "max_words=10000 # N-most common words\n",
    "embedding_dim = 50 \n",
    "\n",
    "tokenizer =Tokenizer (num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index)) # Found 88584 unique tokens.\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels =np.asarray(labels) # \n",
    "print('Shape of data tensor:' , data.shape) #data.shape=(25000,100)\n",
    "print('Shape of label tensor:' , labels.shape) #labels.shape= (25000,)\n",
    "\n",
    "# shuffleing\n",
    "indices = np.arange(data.shape[0]) #data.shape[0]=25000\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# end shuffeling\n",
    "\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data  [training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# khandane file glove\n",
    "# read  file glove r\n",
    "glove_dir='./datasets/glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f=open(os.path.join(glove_dir , 'glove.6b.50d.txt')) #50d har kalame be bordare 50 tai map shode\n",
    "i=0\n",
    "for line in f: # select line by line in each file\n",
    "    i+=1\n",
    "    values=line.split() \n",
    "    word = values[0] \n",
    "    embeddings_index[word]= np.asarray(values[1:],dtype='float32') \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084de5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoÿ£\\xa0ng -0.021476 -0.46788 -0.57489 0.25384 -0.83486 -1.2622 0.74499 0.41843 0.15076 -0.3762 0.31586 1.0779 0.26601 -0.84584 -0.21545 -0.30827 -1.0899 -0.53668 2.0235 1.3613 -0.9779 -0.80162 0.68561 -0.18649 -0.5215 0.8261 -0.1142 -0.37842 -0.70749 -0.082986 -0.54944 -0.25965 0.39062 1.385 0.35108 0.75802 0.064603 1.0825 -0.023899 -0.59102 0.16235 -0.25107 0.20679 -0.12571 -0.018305 -0.91191 -0.27518 -1.3942 0.9766 -0.11565\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e1ba8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 50 \n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items(): # word_index = tokenizer.word_index, dictionary word:index \n",
    "    if i < max_words: #max_words=10000\n",
    "        embedding_vector = embeddings_index.get(word) # dic.get(index)=value=dic[index]\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b628cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#freezing e embedding layer:\n",
    "model.layers[0].set_weights([embedding_matrix]) # layers[0] = Embedding layer,  freez \n",
    "model.layers[0].trainable= False # freeze the embedding layer in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model_history = model.fit(x_train, y_train,\n",
    "                          epochs=10, batch_size=32,\n",
    "                          validation_data=[x_val, y_val])\n",
    "\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e276648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on test:\n",
    "\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "texts = [] \n",
    "labels = [] \n",
    "\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name=os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)): \n",
    "        \n",
    "        if fname[-4:] == '.txt': \n",
    "            f = open (os.path.join(dir_name , fname), errors=\"ignore\") \n",
    "            texts.append(f.read()) \n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "            \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)\n",
    "model.load_weights('pre_trained_golve_model.h5')\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aafc45",
   "metadata": {},
   "source": [
    "# RNN, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16bf74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(10000,32)) #9999-most common words, \n",
    "model.add(SimpleRNN(64, return_sequences=True)) \n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32)) # \n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79b9cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(10000,32))\n",
    "model.add(LSTM(32)) # 32 tedade neuronha\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3772d32ab755bbbd514bc84ca727c2c4f9caf015a60ea1fe9de033ff12031159"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
