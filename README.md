# pre-trained-word-embeddings

word emmbeding: Word2Ves,

dl link IMDB raw_database: https://ai.stanford.edu/~amaas/data/sentiment/

dl link e GloVe word embeding: nlp.stanford.edu/projects/glove/

2 class (neg and pos), 25000 train data (12500 each class), 25000 test data (12500 each class), common words pas feature size =10000

I used Dense, RNN, LSTM models.
